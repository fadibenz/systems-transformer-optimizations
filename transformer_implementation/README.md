# Transformer Implementation

This folder contains an implementation from scratch of a language model, including 
all building blocks for LlaMa-style model: 

- Multi-head Attention
- RoPE
- RMSNorm 
- SwiGLU FFN

I also implement the AdamW optimizer, gradient clipping, cosine scheduler and data loading (with random sampling).

For more details, refer to my original GitHub repo: [https://github.com/fadibenz/Transformerbrrrr]
